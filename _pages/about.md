---
permalink: /
title: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

This is Jiahao Li's home on the web!

I'm a final year PhD student at <a href='http://en.ustc.edu.cn/'>University of Science and Technology of China</a>, supervised by Prof. <a href='https://scholar.google.com/citations?user=hxGs4ukAAAAJ&hl=en&oi=ao'>Yongdong Zhang</a>. and Prof. <a href='https://scholar.google.com/citations?hl=zh-CN&user=m-0P8sgAAAAJ'>Zhendong Mao</a>. I also closely collaborate with  <a href='https://scholar.google.com/citations?user=l2yEbhAAAAAJ&hl=zh-CN&oi=ao'>Quan Wang</a>.
My research interests lie broadly in natural language processing, especially post-training for LLMs. <br/>

<!-- My **CV** can be downloaded *<a href="assets/JiahaoLi_CV.pdf">here</a>*. -->

# üìñ Educations
- *2020.09 - 2026.06 (Expected)*, **PhD Student** in Electronic Information, *University of Science and Technology of China*
  - Supervised by Prof. Yongdong Zhang and Prof. Zhendong Mao
- *2016.09 - 2020.06*, **Bachelor of Engineering** in Cyberspace Security, *University of Science and Technology of China*  

# üìù Publications 


[Feature-Adaptive and Data-Scalable In-Context Learning](https://aclanthology.org/2024.acl-long.81.pdf) [[**Code**](https://github.com/jiahaozhenbang/FADS-ICL)]

**Jiahao Li**, Quan Wang, Licheng Zhang, Guoqing Jin, Zhendong Mao*




-  Due to context length constraints, it cannot be further improved in spite of more training data, and general features directly from LLMs in ICL are not
adaptive to the specific downstream task.
-  In this paper, we propose a feature-adaptive and data-scalable in-context learning framework (FADS-ICL), which can leverage task-adaptive features
to promote inference on the downstream task, with the supervision of beyond-context samples.



[Grammatical Error Correction via Mixed-Grained Weighted Training](https://aclanthology.org/2023.findings-emnlp.400.pdf)

**Jiahao Li**, Quan Wang‚àó, Chiwei Zhu, Zhendong Mao, Yongdong Zhang


-  Almost all previous works for GEC treat annotated training data equally, but inherent discrepancies in data are neglected. 
- In this paper, we propose MainGEC, which designs token-level and sentence-level training weights based on inherent discrepancies in accuracy and potential diversity of data
annotation, respectively, and then conducts mixed-grained weighted training to improve the training effect for GEC.



[Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity](https://aclanthology.org/2022.emnlp-main.287/) [[**Code**](https://github.com/jiahaozhenbang/SCOPE)]

**Jiahao Li**, Quan Wang*, Zhendong Mao, Junbo Guo, Yanyan Yang, Yongdong Zhang



- As most of these spelling errors are caused by phonetic similarity, effectively modeling the pronunciation of Chinese characters is a key factor for Chinese Spelling Check (CSC).
-  In this paper, we consider introducing an auxiliary task of Chinese pronunciation prediction (CPP) to improve CSC, and, for the first time, systematically discuss the adaptivity and granularity of this auxiliary task.


# üéñ Honors and Awards
- *2020* Language and Intelligent Technology Competition: The 4th Place of Machine Reading Comprehension Task.
- *2019* USTC Outstanding Student Scholarship Gold Award.
- *2018* USTC Outstanding Student Scholarship Silver Award.


# üìö Experience
- *2022.9-2023.3*, Research Intern, State Key Laboratory of Communication Content Cognition.
- *2022.5-2022.8*, Participated in the intelligent production project of LCFC (Hefei) Electronics Technology Co., Ltd. 
- *2019.7-2019.9*, Summer project (The detection and tracking in team sports) in University of Technology Sydney, Australia. (supervised by Assoc. Prof. Min Xu). 

